{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Institutional Network from Microsoft Academic Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geocode Affiliations\n",
    "\n",
    "Attempt to automatically geocode institutions. Not all institutions will be able to be found and geocoded by our chosen (free) geocoding software. And some results will simply be wrong. To view the output of the code in the cell below, take a look at `geodata/geocoded.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import googlemaps\n",
    "\n",
    "geocoded_data = {}\n",
    "\n",
    "with open(\"geodata/geocoded.csv\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        place = row[\"place\"]\n",
    "        geocoded_data[place] = row\n",
    "        del geocoded_data[place][\"place\"]\n",
    "        \n",
    "gmaps = googlemaps.Client(key='[insert key]')\n",
    "path_to_data = \"sourcedata/mag_sdg.csv\"\n",
    "\n",
    "with open(path_to_data) as datafile:\n",
    "    \n",
    "    reader = csv.DictReader(datafile)\n",
    "    for row in reader:\n",
    "        place = row[\"affiliation_name\"]\n",
    "        if place not in geocoded_data:\n",
    "            \n",
    "            try:\n",
    "                # Take first result with [0]\n",
    "                geocode_result = gmaps.geocode(place)[0]\n",
    "                country = None\n",
    "                for section in geocode_result[\"address_components\"]:\n",
    "                    if \"country\" in section[\"types\"]:\n",
    "                        country = section[\"long_name\"]\n",
    "                \n",
    "                locdata = {\n",
    "                    \"address\": geocode_result[\"formatted_address\"],\n",
    "                    \"latitude\": geocode_result[\"geometry\"][\"location\"][\"lat\"],\n",
    "                    \"longitude\": geocode_result[\"geometry\"][\"location\"][\"lng\"],\n",
    "                    \"country\": country \n",
    "                }\n",
    "                geocoded_data[place] = locdata\n",
    "                print(place, locdata[\"country\"])\n",
    "                \n",
    "            except Exception as e:\n",
    "                geocoded_data[place] = \"Null\"\n",
    "                print(\"====\", e)\n",
    "                print(\"====\", f\"Unable to locate {place}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4705"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(geocoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([g for g in geocoded_data if geocoded_data[g] == \"Null\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Output data to file.\n",
    "with open(\"geodata/geocoded.csv\", \"w\") as f:\n",
    "    \n",
    "    writer = csv.DictWriter(f, fieldnames=[\"place\", \"address\", \"latitude\", \"longitude\", \"country\"])\n",
    "    writer.writeheader()\n",
    "    for g, data in geocoded_data.items():\n",
    "        # print(type(data))\n",
    "        if data == \"Null\":\n",
    "            data = {\"address\": \"\", \"latitude\": \"\", \"longitude\": \"\", \"country\": \"\"}\n",
    "        data[\"place\"] = g\n",
    "        writer.writerow(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a data file that lists countries and continents ([Source](https://datahub.io/JohnSnowLabs/country-and-continent-codes-list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"sourcedata/country-and-continent-codes-list.json\") as f:\n",
    "    countries = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a lookup devices for finding countries by name, then countries and other geo-information by affiliation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries_by_name = {}\n",
    "for country in countries:\n",
    "    country_name = country[\"Country_Name\"].split(\",\")[0].strip()\n",
    "    countries_by_name[country_name] = country\n",
    "    \n",
    "with open(\"sourcedata/countries_by_name.json\", \"w\") as f:\n",
    "    json.dump(countries_by_name, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, `geocoded.csv` has been updated manually with country names not found by the Google geocoder (in this case about 250 values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "geo_by_affiliation = {}\n",
    "with open(\"geodata/geocoded.csv\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        geo_by_affiliation[row[\"place\"]] = row\n",
    "        \n",
    "import json\n",
    "with open(\"geodata/geo_by_affiliation.json\", \"w\") as f:\n",
    "    json.dump(geo_by_affiliation, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting data from CSV\n",
    "\n",
    "Fill out several dictionaries with data from the csv file.\n",
    "* **data_by_id**: article information keyed by article id.\n",
    "* **data_by_inst**: data about institutions keyed by the name of the institution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "\n",
    "data_by_id = {}\n",
    "data_by_inst = {}\n",
    "\n",
    "missing_country_values = {\n",
    "    \"Kashmir\": \"Asia\",\n",
    "    \"Africa\": \"Africa\"\n",
    "}\n",
    "\n",
    "with open(\"geodata/geo_by_affiliation.json\") as f:\n",
    "    geo_by_affiliation = json.load(f)\n",
    "    \n",
    "with open(\"sourcedata/countries_by_name.json\") as f:\n",
    "    countries_by_name = json.load(f)\n",
    "\n",
    "with open(\"sourcedata/mag_sdg.csv\") as datafile:\n",
    "    \n",
    "    reader = csv.DictReader(datafile)\n",
    "    \n",
    "    for row in reader:\n",
    "        \n",
    "        id_ = row[\"paper_id\"]\n",
    "        author_id = row[\"author_id\"]\n",
    "        author = row[\"author_name\"]\n",
    "        inst = row[\"affiliation_name\"]\n",
    "        \n",
    "        if id_ not in data_by_id:\n",
    "            data_by_id[id_] = [inst]\n",
    "        else:\n",
    "            if inst not in data_by_id[id_]:\n",
    "                data_by_id[id_].append(inst)\n",
    "        \n",
    "        if inst not in data_by_inst:\n",
    "            data_by_inst[inst] = {\n",
    "                \"affiliation_id\": [row[\"affiliation_id\"]],\n",
    "                \"affiliation_name\": [row[\"affiliation_name\"]],\n",
    "                \"author_name\": [row[\"author_name\"]],\n",
    "                \"doi\": [row[\"doi\"]] if row[\"doi\"] != \"\" else [],\n",
    "                \"paper_id\": [row[\"paper_id\"]],\n",
    "                \"year\": [row[\"year\"]],                \n",
    "            }\n",
    "            if inst in geo_by_affiliation:\n",
    "                country = geo_by_affiliation[inst][\"country\"]\n",
    "            else:\n",
    "                country = \"\"\n",
    "                print(\"Missing institution\", inst)\n",
    "                \n",
    "            if country != \"\":\n",
    "                if country in countries_by_name:\n",
    "                    continent = countries_by_name[country][\"Continent_Name\"]\n",
    "                else:\n",
    "                    print(\"Missing country\", country)\n",
    "                    continent = missing_country_values.get(country, \"Unknown\")\n",
    "            else:\n",
    "                print(\"Missing country\", country)\n",
    "                country = \"Unknown\"\n",
    "                continent = \"Unknown\"\n",
    "            \n",
    "            data_by_inst[inst][\"country\"] = country\n",
    "            data_by_inst[inst][\"continent\"] = continent\n",
    "        \n",
    "        else:\n",
    "            doi = row[\"doi\"]\n",
    "            if doi != \"\":\n",
    "                data_by_inst[inst][\"doi\"].append(doi)\n",
    "            for key in [\"affiliation_id\", \"affiliation_name\", \"author_name\", \"paper_id\", \"year\"]:\n",
    "                data_by_inst[inst][key].append(row[key])\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check values for accuracy / highlight obvious issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['affiliation_id', 'affiliation_name', 'author_name', 'doi', 'paper_id', 'year', 'country', 'continent'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_by_inst['university of tasmania'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'university of tasmania'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data_by_inst.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_by_inst['university of tasmania']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building graph from institution and article data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create the complete graph of all institutional collaborations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for id_, data in data_by_id.items():\n",
    "    \n",
    "    for i, j in combinations(data, 2):\n",
    "\n",
    "        if G.has_edge(i, j):\n",
    "            G[i][j][\"weight\"] += 1\n",
    "            if id_ not in G[i][j][\"articles\"]:\n",
    "                G[i][j][\"articles\"].append(id_)\n",
    "            \n",
    "        else:\n",
    "            # Attempt to determine 'scale' of geographic collaboration.\n",
    "            if data_by_inst[i][\"country\"] == \"Unknown\" or data_by_inst[j][\"country\"] == \"Unknown\":\n",
    "                connection_type = \"Unknown\"\n",
    "            elif data_by_inst[i][\"continent\"] != data_by_inst[j][\"continent\"]:\n",
    "                connection_type = \"trans-continental\"\n",
    "            elif data_by_inst[i][\"country\"] != data_by_inst[j][\"country\"]:\n",
    "                connection_type = \"international\"\n",
    "            elif data_by_inst[i][\"country\"] == data_by_inst[j][\"country\"]:\n",
    "                connection_type = \"domestic\"\n",
    "                        \n",
    "            G.add_edge(i, j, weight=1, articles=[id_], connection=connection_type)\n",
    "\n",
    "for inst, data in G.nodes(data=True):\n",
    "    data[\"affiliation\"] = data_by_inst[inst][\"affiliation_name\"][0]\n",
    "    data[\"affiliation_label\"] = data_by_inst[inst][\"affiliation_name\"][0].title()\n",
    "    data[\"papers\"] = \"<br>\\n\".join(data_by_inst[inst][\"paper_id\"])\n",
    "    data[\"dois\"] = \"<br>\\n\".join(data_by_inst[inst][\"doi\"])\n",
    "    data[\"name\"] = \"<br>\\n\".join(list(set(data_by_inst[inst][\"author_name\"])))    \n",
    "    data[\"count\"] = len(data_by_inst[inst][\"paper_id\"])\n",
    "    data[\"country\"] = data_by_inst[inst][\"country\"]\n",
    "    data[\"continent\"] = data_by_inst[inst][\"continent\"]\n",
    "    data[\"domestic\"] = len([x for x in G[inst] if G[inst][x][\"connection\"] == \"domestic\"])\n",
    "    data[\"international\"] = len([x for x in G[inst] if G[inst][x][\"connection\"] \n",
    "                 in [\"trans-continental\", \"international\"]])\n",
    "    data[\"domestic_pct\"] = int(round(data[\"domestic\"] / (data[\"domestic\"] + data[\"international\"]) * 100))\n",
    "    data[\"international_pct\"] = int(round(data[\"international\"] / (data[\"domestic\"] + data[\"international\"]) * 100))\n",
    "    data[\"degree\"] = data[\"domestic\"] + data[\"international\"]\n",
    "    max_year = max(data_by_inst[inst][\"year\"])\n",
    "    min_year = min(data_by_inst[inst][\"year\"])\n",
    "    if max_year == min_year:\n",
    "        data[\"year\"] = max_year\n",
    "    else:\n",
    "        data[\"year\"] = \"{0}–{1}\".format(min_year, max_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20411"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(G.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3454"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(G.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create subgraph containing only those edges with a `weight` greater than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_edge(n1, n2):\n",
    "    \"\"\"Check if weight is larger than 1.\"\"\"\n",
    "    return G[n1][n2][\"weight\"] > 1\n",
    "\n",
    "def filter_node(n):\n",
    "    \"\"\"Filter out unconnected nodes.\"\"\"\n",
    "    return not nx.is_isolate(view, n)\n",
    "\n",
    "view = nx.subgraph_view(G, filter_edge=filter_edge)\n",
    "subview = nx.subgraph_view(view, filter_node=filter_node)\n",
    "\n",
    "for inst, data in subview.nodes(data=True):\n",
    "    \n",
    "    data[\"domestic\"] = len([x for x in subview[inst] if subview[inst][x][\"connection\"] == \"domestic\"])\n",
    "    data[\"international\"] = len([x for x in subview[inst] if subview[inst][x][\"connection\"] \n",
    "                 in [\"trans-continental\", \"international\"]])\n",
    "    data[\"domestic_pct\"] = int(round(data[\"domestic\"] / (data[\"domestic\"] + data[\"international\"]) * 100))\n",
    "    data[\"international_pct\"] = int(round(data[\"international\"] / (data[\"domestic\"] + data[\"international\"]) * 100))\n",
    "    data[\"degree\"] = data[\"domestic\"] + data[\"international\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2389, 828)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subview.edges()), len(subview.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(nx.isolates(subview)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for e1, e2 in subview.edges():\n",
    "\n",
    "    weight = subview[e1][e2][\"weight\"]\n",
    "    if weight > 4:\n",
    "        print(\"-------------\")\n",
    "        print(e1, e2)\n",
    "        print(subview[e1][e2][\"weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for auth_id, auth_data in data_by_author.items():\n",
    "    if any(len(auth_data[key]) > 2 for key in auth_data.keys()):\n",
    "        print(auth_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'420720'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data_by_id.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kasetsart university',\n",
       " 'hiroshima shudo university',\n",
       " 'university of shiga prefecture']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_by_id[\"524975\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output graph data as separate files containing `nodes` and `edges` for display in `flourish.studio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "nx.write_edgelist(subview, \"mag_output_inst/mag_inst_edges.tsv\", delimiter=\"\\t\", data=[\"weight\", \"connection\"])\n",
    "with open(\"mag_output_inst/mag_inst_nodes.csv\", \"w\") as f:\n",
    "    fieldnames = [\"id\", \"country\", \"continent\", \"affiliation\", \"papers\", \"name\", \"year\", \"count\",\n",
    "                 \"affiliation_label\", \"dois\", 'degree', 'domestic', 'domestic_pct', 'international_pct',\n",
    "                  'international']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for n, data in list(subview.nodes(data=True)):\n",
    "        row = data\n",
    "        row[\"id\"] = n\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([subview[e][v][\"weight\"] for e, v in subview.edges])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centrality measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cc = nx.closeness_centrality(subview)\n",
    "print(cc)\n",
    "bc = nx.betweenness_centrality(subview)\n",
    "print(bc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dc = nx.degree_centrality(subview)\n",
    "ec = nx.eigenvector_centrality(subview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from pprint import pprint\n",
    "\n",
    "measure = ec\n",
    "\n",
    "measure_sorted = sorted(measure.items(), key=itemgetter(1), reverse=True)\n",
    "pprint(measure_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "centralities = []\n",
    "\n",
    "insts = list(ec.keys())\n",
    "\n",
    "with open(\"mag_output_inst/centrality_by_institution.csv\", \"w\") as f:\n",
    "    \n",
    "    writer = csv.DictWriter(f, fieldnames=[\"institution\", \"betweenness\", \"closeness\", \"degree\", \"eigenvector\"])\n",
    "    writer.writeheader()\n",
    "    for i in insts:\n",
    "\n",
    "        data = {\n",
    "            \"institution\": i,\n",
    "            \"betweenness\": bc[i],\n",
    "            \"closeness\": cc[i],\n",
    "            \"degree\": dc[i],\n",
    "            \"eigenvector\": ec[i]\n",
    "        }\n",
    "        writer.writerow(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
